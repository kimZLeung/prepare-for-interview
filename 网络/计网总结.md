# 计网总结

## 物理层



## 数据链路层

### 三个基本问题

- 封装成帧
- 透明传输
- 差错检测



### PPP帧与Mac帧的区别

> ppp属于广域网范畴，MAC是局域网范畴，按实际情况和环境就选用不同的协议，ppp支持的网络结构只能是点对点，mac支持多点对多点。
>
> 以太网中用mac，远程的话就用ppp（如ADSL拨号，就是基于ppp的）。
>
> ppp是点到点协议  ,逻辑上相连的就一台设备，因此不需要寻址, 目标地址为广播地址, PPP中前6个字节就是目标地址。



## 网络层

> 提供主机间的逻辑通信



### ARP

ARP是解决**同一个局域网**中主机或路由器IP地址到硬件地址的映射问题。

> 每台主机都有一个ARP高速缓存，里面存放着一个从IP地址到硬件地址的映射表，并且动态更新

步骤：

- ARP进程在局域网上广播发送一个ARP请求分组。（附带上自己的IP地址和硬件地址以及目的地址）
- 在局域网上所有主机上运行ARP进程都会收到这个ARP分组
- 主机B的IP地址和ARP请求分组中要查询的一致，就收下这个请求分组，并且向主机A发送ARP响应分组
- 主机A收到主机B的ARP响应分组后，在自己的ARP高速缓存中写入主机B的IP地址到硬件地址的映射



### ICMP

ICMP报文是装在IP数据报中的，作为其中的数据部分。

应用：

- ping
- tracerouter



### 路由选择协议

#### 内部网关协议IGP

**RIP**：基于距离向量

优点：

- 配置简单
- 开销小



缺点

- **大量广播**。RIP向所有邻居每隔30秒广播一次完整的路由表,将占用宝贵的带宽资源,在较慢的广域网链路上尤其有问题｡
- **没有成本概念**｡RIP没有网络延迟和链路成本的概念｡当采用RIP时,路由/转发的决定只是基于跳线,这样,很容易导致无法选择最佳路由｡例如,一条链路拥有较高的带宽,但是,跳数较多,从而不能被选择｡
- **支持的网络规模有限**｡由于RIP路由协议最多只支持16个步跳,当超过该跳数时,网络将认为无法到达｡因此,RIP只能适用于规模较少的网络｡



**OSPF**:最短路径优先

优点：

- **快速收敛**｡OSPF是真正的LOOP- FREE(无路由自环)路由协议｡源自其算法本身——链路状态及最短路径树算法,OSPF收敛速度快,能够在最短的时间内将路由变化传递到整个自治系统｡
- **区域划分**｡提出区域(Area)划分的概念,将自治系统划分为不同区域后,通过区域之间的对路由信息的摘要,大大减少了需传递的路由信息数量,也使得路由信息不会随网络规模的扩大而急剧膨胀｡
- **开销控制**｡将协议自身的开销控制到最小｡用于发现和维护邻居关系的是定期发送的不含路由信息的hello报文,非常短小｡包含路由信息的报文是触发更新的机制,而且只有在路由变化时才会发送｡但为了增强协议的健壮性,每1800秒全部重发一次｡
- **安全性高**｡良好的安全性,OSPF支持基于接口的明文及MD5 验证｡



缺点：

- **配置相对复杂**｡由于网络区域划分和网络属性的复杂性,需要网络分析员有较高的网络知识水平才能配置和管理OSPF网络｡
- **路由负载均衡能力较弱**｡OSPF虽然能根据接口的速率､连接可靠性等信息,自动生成接口路由优先级,但在通往同一目的的不同优先级路由中,OSPF只选择优先级较高的转发,不同优先级的路由中,不能实现负载分担｡只有相同优先级的,才能达到负载均衡的目的

#### 外部网关协议EGP

**BGP**

> 高度抽象，把AS自治系统抽象成一个路由，它就是一个基于路径向量路由选择协议。每个AS系统会有一个或者多个路由器充当发言人，根据收到的路由信息拓扑出AS的树型结构连通图



### NAT



## 运输层

> 提供应用进程间端到端的逻辑通信



### UDP

- 无连接，不可靠，面向报文
- 无拥塞控制，支持一对一，一对多，多对多，多对一，首部开销小
- 检验和把首部和数据部分一起都检验

#### UDP丢包

**丢包的主要原因**

1. 接收端处理时间过长导致丢包：调用recv方法接收端收到数据后，处理数据花了一些时间，处理完后再次调用recv方法，在这二次调用间隔里，发过来的包可能丢失。**对于这种情况可以修改接收端，将包接收后存入一个缓冲区，然后迅速返回继续recv.**
2. 发送的包较大，超过接受者缓存导致丢包：包超过mtu size数倍，几个大的udp包可能会超过接收者的缓冲，导致丢包
3. 发送的包频率太快：虽然每个包的大小都小于mtu size 但是频率太快

**解决方案**

- 可以修改接收端，将包接收后存入一个缓冲区
- 对于超过缓存大小的包，可以选择直接接受
- 通过流量控制



### TCP

- 面向连接，可靠
- 面向字节流
- 全双工通信




#### TCP UDP区别

- TCP 面向连接而 UDP 是无连接的
- TCP 提供可靠交付(无差错、不丢失、不重复、并且按序到达)，UDP 使用尽最大努力交付
- TCP 连接只能是一对一而 UDP 支持一对一、一对多、多对一和多对多的交互通信
- TCP 面向字节流而 UDP 是面向报文的
- TCP 有拥塞控制，而 UDP 没有拥塞控制
- TCP 因为拥塞控制，速率降低，但 UDP 不会，没有太大时延




#### TCP粘包拆包

- **粘包拆包出现的原因** ：  因为TCP面向字节流传输的，发送端需要等缓冲区满才发送出去，造成粘包。接收方不及时接收缓冲区的包，造成多个包接收    
- **非工程性的解决方法**：
  - 利用TCP提供了强制数据立即传送的操作指令push 
    **缺点**：但它关闭了优化算法，降低了网络发送效率，影响应用程序的性能
  - 精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据
    **缺点**：只能减少出现粘包的可能性，但并不能完全避免粘包。因为当发送频率较高时，或由于网络突发可能使某个时间段数据包到达接收方较快，接收方还是有可能来不及接收，从而导致粘包。
  - 两次send函数之间添加 sleep函数
    **缺点**：会降低数据传输效率
- **工程型的解决方法**：
  - 1.消息定长，例如每个报文的大小为固定长度200字节,如果不够，空位补空格；
  - 2.在包尾增加回车换行符进行分割（模仿帧的设计方式）；
  - 3.将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段，通常设计思路是消息头的第一个字段用int来表示消息的总长度；（在pushsdk中使用此种方式利用了Netty中的LengthFieldBasedFrameDecoder解码器实现)
  - 4.更复杂的应用层协议；
  - 添加标志字段，在每次发送数据是添加标记字段：A： =>size 标记数据长度的方式  B：特定标记字段标记数据的结尾（模仿帧的设计方式）＝>结束符的方式
  - 定义应用层的数据通讯协议 ：=>如果数据按照一定的方式存储或着优加密的需求， 可以通过自己定制 数据通讯协议对数据封装，并实现自己的数据 封包｜ 拆包函数。

> 为了解决TCP粘包拆包的问题，Netty默认提供了多种编码器来处理





#### 根据MTU拆分成多个数据报

http://blog.csdn.net/yusiguyuan/article/details/22782943

（TCP层的分段和IP层的分片之间的关系 & MTU和MSS之间的关系）



#### 三次握手

![理论联系实际：Wireshark抓包分析TCP 3次握手、4次挥手过程_6.jpg](http://www.52im.net/data/attachment/forum/201605/02/132229ti2rqg2lpnik3ii1.jpg)

在客户端和服务端都创建好传输控制模块TCB的条件下：

- 客户端向服务端发出连接请求报文段


- 服务端接受到连接请求报文段后，同意建立连接就返回一个确认报文段
- 客户端的TCP客户进程收到确认后，还需要向服务端发出确认报文段

> ps:**传输控制模块TCB**:存储了每一个连接中的一些重要信息，比如TCP连接表，重传队列指针，当前发送的序列号等等。



为什么是三次握手？

> **为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误**

会有这样的一种情况：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。**本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。**采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。”。**主要目的防止server端一直等待，浪费资源。**



#### 四次挥手

![理论联系实际：Wireshark抓包分析TCP 3次握手、4次挥手过程_15.jpg](http://www.52im.net/data/attachment/forum/201605/02/132717xa86zoapkfmhmppz.jpg)

- 1.客户端发送连接释放报文段，并且停止发送数据，主动关闭TCP连接，然后进入等待状态，等待服务端的确认
- 2.服务端收到连接释放报文段之后，立即发送确认报文段，然后进入关闭等待状态。并且TCP服务器进程会通知高层应用进程。这个时候，客户端到服务端这个方向的连接就处于半关闭状态。
- 客户端收到确认报文段之后，等待服务端发出连接释放报文段
- 3.若服务端没有要向客户端发送的数据之后，应用进程会通知TCP释放连接。这个时候，服务端会发出连接释放报文段
- 4.客户端收到连接释放报文段之后，会返回一个确认报文段。然后自己进入时间等待状态，即等待一个时间才会最终进入关闭状态。
  - 为什么需要等待一个时间？
    - 一、为了保证客户端最后发送的一个报文段能够到达服务端
    - 二、防止已失效的连接请求报文段出现在本连接中
- 服务端只要收到了客户端最后一次的确认报文段就会进入关闭状态，结束这次TCP连接



为什么需要四次挥手？

> **确保数据能够完成传输**。 假设服务端收到连接释放报文段之后马上就释放连接，那么服务端还存在数据需要发送给客户端的话，就不能保证这些数据能够发送到客户端身上。因此需要等待服务端把数据发送完之后，由服务端发起一条连接释放报文段，也就是第三次挥手，最后由客户端发起确认，也就是第四次挥手，因此需要四次挥手才能确保数据能够完成完整的传输。



#### 可靠传输的实现

[TCP可靠传输的实现](http://www.cnblogs.com/newwy/p/3238362.html)

> TCP为了提供可靠传输：

> （1）首先，采用三次握手来建立TCP连接，四次握手来释放TCP连接，从而保证建立的传输信道是可靠的。
>
> （2）其次，TCP采用了连续ARQ协议（回退N，Go-back-N；超时自动重传）来保证数据传输的正确性，使用滑动窗口协议来保证接方能够及时处理所接收到的数据，进行流量控制。
>
> （3）最后，TCP使用**慢启动**、**拥塞避免**、**快重传**和**快恢复**来进行拥塞控制，避免网络拥塞。

- 以字节为单位的滑动窗口
- 缓存机制
  - 发送缓存用来暂时存放：
    1.发送应用程序传送给发送方TCP准备的数据
    2.TCP已发送但尚未收到确认的数据。
  - 接收缓存用来暂时存放：
    1.按序到达的，但尚未被接收应用程序读取的数据。
    2.未按序到达的数据。
- 超时重传的时间选择



#### TCP流量控制原理

   所谓流量控制就是让发送发送速率不要过快，让接收方来得及接收。利用滑动窗口机制就可以实施流量控制。

​         原理这就是**运用TCP报文段中的窗口大小字段来控制，发送方的发送窗口不可以大于接收方发回的窗口大小。**

​         考虑一种特殊的情况，就是接收方若没有缓存足够使用，就会发送零窗口大小的报文，此时发送放将发送窗口设置为0，停止发送数据。之后接收方有足够的缓存，发送了非零窗口大小的报文，但是这个报文在中途丢失的，那么发送方的发送窗口就一直为零导致死锁。

​         解决这个问题，TCP为每一个连接设置一个持续计时器（persistence timer）。只要TCP的一方收到对方的零窗口通知**，就启动该计时器，周期性的发送一个零窗口探测报文段。对方就在确认这个报文的时候给出现在的窗口大小（注意：**TCP规定，即使设置为零窗口，也必须接收以下几种报文段：零窗口探测报文段、确认报文段和携带紧急数据的报文段）。



#### 拥塞控制

慢开始和拥塞避免

![img](http://static.oschina.net/uploads/space/2016/0112/214351_J88Z_1403215.png)

ps：
**拥塞窗口**：cwnd**（Congestion Window)是发送端**根据自己估计的网络拥塞程度而设置的窗口值，是来自发送端的流量控制。

如果发现RTT在增大，Vegas就认为网络正在*发生拥塞*



（1）**慢启动原理**

​     1）当主机开始发送数据时，如果立即将较大的发送窗口的全部数据字节都注入到网络中，那么由于不清楚网络的情况，有可能引其网络拥塞

​     2）比较好的方法是试探一下，即从小到达逐渐增大发送端的拥塞控制窗口数值

​     3）通常在刚刚开始发送报文段时可先将拥塞窗口cwnd(拥塞窗口)设置为一个最大报文段的MSS的数值。在每收到一个对新报文段确认后，将拥塞窗口增加至多一个MSS的数值，当rwind（接收窗口）足够大的时候，为了防止拥塞窗口cwind的增长引起网络拥塞，还需要另外一个变量---慢开始门限ssthresh



（2）**拥塞避免原理**

​	1）让拥塞窗口cwnd缓慢增大，每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。按线性规律缓慢增长。



> 无论，在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞之后，就会把门限ssthresh设置为发送方发送窗口值的一半



---



**快重传和快恢复**

**快重传算法**：在某些情况下更早的重传丢失的报文段（如果当发送端接收到三个重复的确认ACK时，则断定分组丢失，立即重传丢失的报文段，而不必等待重传计时器超时）。

例如：M1，M2，M3 -----> M1,M3,缺失M2，则接收方向发送方持续发送M2重复确认，当发送方收到M2的三次重复确认，则认为M2报文丢失，启动快重传机制，重传数据，其他数据发送数据放入队列，待快重传结束后再正常传输。



**快恢复算法**有以下两个要点：

​     1）当发送方连续收到接收方发来的三个重复确认时，就执行“乘法减小”算法，把慢开始门限减半，这是为了预防网络发生拥塞。

​     2）由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，而是把**cwnd(拥塞窗口)**值设置为慢开始门限减半后的值，然后开始执行拥塞避免算法**，使拥塞窗口的线性增大**。



>  ps: **超时重传**是TCP协议保证数据可靠性的另一个重要机制，其原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。



## 应用层

> 协议规定应用进程间标准

### DNS

> 域名解析成IP地址

域名服务器间的查询

- 迭代查询
- 递归查询

域名服务器广泛使用高速缓存，减轻根域名服务器以及减少因特网上DNS查询报文数量



### DHCP

**整体流程**：主机A以广播形式发送报文，DHCP中继（通常是一个路由器）收到报文之后，以单播形式转发此报文给DHCP服务器，然后又通过中继回发提供报文给主机A。DHCP客户就可以使用被分配的IP地址，但是有一定的租用期



详细的工作步骤：

- DHCP服务器被动打开UDP端口67，等待客户端发送报文
- DHCP客户从UDP端口68发送DHCP发现报文
- 凡是收到DHCP发现报文的DHCP服务器都发出DHCP提供报文（DHCP客户端可以收到多个DHCP提供报文）
- DHCP从几个DHCP服务器中选择一个，并向选择的DHCP服务器发送DHCP请求报文
- 被选择的DHCP服务器发送确认报文。这个时候，DHCP客户就可以使用IP地址了。这是一种**已绑定**的状态，即DHCP客户的IP地址和硬件地址已经绑定了。这个时候DHCP客户就会根据服务器提供的租用期设置两个计时器，分别在0.5T和0.85T的时候，请求更新租用期。
- 租用期过了一半，DHCP就会发送请求报文要求更新租用期
- DHCP服务器若同意，则发回确认报文，得到新的租用期，重新设置计时器
- 若不同意，则发回否认报文。这时DHCP客户必须立即停止使用原来的IP地址，然后重新申请新的IP地址。
- 若服务器不响应，则在0.85T的时候重新执行以上操作
- DHCP客户可以随时提前终止服务器所提供的租用期，只需要发送释放报文即可。



### HTTP

> HTTP协议是无状态的，即同一个客户第二次访问同一个服务器上的页面的时候，服务器响应时间与第一次响应时间相同。

HTTP请求报文是作为TCP三次握手的第三个报文的数据



#### HTTP/0.9

- 只有一个命令GET
- 服务端响应统一为HTML格式的字符串

单纯可以用来渲染HTML页面



#### HTTP/1.0

增加

- 命令
- 请求回应格式变化
- 除了数据部分，每次通信都必须包括头信息（HTTP header）

```js
GET / HTTP/1.0
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)
Accept: */*
```

```
HTTP/1.0 200 OK 
Content-Type: text/plain
Content-Length: 137582
Expires: Thu, 05 Dec 1997 16:00:00 GMT
Last-Modified: Wed, 5 August 1996 15:55:28 GMT
Server: Apache 0.84

<html>
  <body>Hello World</body>
</html>
```



非持续连接：每次请求完就会关闭TCP连接

缺点：

- 每请求一个文档就需要两倍RTT的开销。
- **每一次建立新的TCP连接**都要分配缓存和变量
  - 现在浏览器都提供了能够打开5-10个并行的TCP连接，每一个TCP连接处理一个客户请求



#### HTTP/1.1

使用持续连接：就是万维网服务器在发送响应后仍然在一段时间内保持这条连接，使同一个客户可以继续在这条连接上传送后续的HTTP报文



两种工作方式：

- 非流水线方式
  - 特点：
  - 客户收到前一个响应后才能发出下一个请求
  - 缺点：
  - 服务器在发送完一个对象后，TCP连接会空闲，浪费资源
- 流水线方式
  - 特点：
  - 客户收到HTTP响应报文之前就可以继续发送新的请求报文。
  - 优点：
  - TCP连接的空闲时间减少



#### 两者区别

（http://blog.csdn.net/forgotaboutgirl/article/details/6936982）

- 可扩展性
- 缓存
- 带宽优化
- 长连接
- 消息传递
- Host头域
- 错误提示
- 内容协商




目前，对于同一个域名，大多数浏览器允许同时建立6个持久TCP连接，也就是说可以同时并发加载6个资源（HTTP/1.1），可以通过以下方式优化：

- 增加并发数
  - 域名分区
- 减少请求数
  - 合并JS脚本和CSS样式表
  - 将图片嵌入CSS代码（data:mediatype; base64,data），使用CSS sprite



#### 顺便补充网页优化：

- 合并JS脚本和CSS样式表
- 将图片嵌入CSS代码（data:mediatype; base64,data），使用CSS sprite
- 静态文件采用 cdn 引入
- 合理使用HTTP的缓存头，利用缓存。
- 对于代码应该考虑性能来编写,比如使用`requestAnimationFrame`绘制动画,尽可能减少页面重绘(DOM 改变)
- `Webpack`打包时分离第三方依赖的chunk`vendor`，异步加载`chunk`，加快首屏渲染，减少打包整体体积
- 域名分区
- 页面做服务端渲染，优化seo，减少客户端处理时间





#### SPDY

**对比起HTTP1.1的改进：**

- 在一个SPDY连接中允许建立多条stream（虚拟流），并发发送多个HTTP请求，请求个数是没有限制的，这里涉及到连接中虚拟流的概念。（多路复用，不用一个一个请求按顺序返回）
- HTTP请求可以具有优先级，也就是说客户端可以要求服务器优先发送重要的资源，这就避免了一个处理时间很长的非关键请求阻塞了服务器对后面请求的处理，影响网页的加载速度。（虽然多路复用可以并行响应，但是并发始终受物理条件控制）
- SPDY协议允许压缩头部，减小HTTP头部的大小，减小对带宽的占用。
- 服务器可以通过`X-Associated-Content` 协议头主动的给客户端发送数据，不需要客户端的主动请求




#### HTTP/2

**二进制格式的报文**

HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制



**多路复用**

HTTP2使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。

当然HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。

**关于多路复用，可以稍微扯上NIO**



**数据压缩**

HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。



**服务器推送**

HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。



#### HTTP和HTTPS的区别

- https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用
- http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议
- http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443
- http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全
- 传输效率上 HTTP 要高于 HTTPS ，因为 HTTPS 需要经过加密过程，过程相比于 HTTP 要繁琐一点，效率上低一些也很正常



#### HTTPS握手过程

- **客户端发出请求（ClientHello）** 
  - 支持的协议版本
  - 支持的加密方法
  - 第一个随机数
- **服务器回应（SeverHello）**
  - 确认使用的加密通信协议版本
  - 确认使用的加密方法
  - 第二个随机数
  - 服务器证书
- **客户端回应** ：先验证证书，如果证书不通过验证，则向用户发出一个警告。如果没有问题，就从证书中取出服务器公钥
  - 第三个随机数（pre-master key），经过服务器公钥加密。（**RSA算法加密**）
  - 编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。
  - 客户端握手结束通知
- **服务器的最后回应**
  - 编码改变通知
  - 服务器握手结束通知
- **开始通信** ，通过三个随机数生成的对称秘钥通过AES算法进行加密数据传输


>  为什么需要三个随机数？
>
> pre master的存在在于SSL协议不信任每个主机都能产生完全随机的随机数，如果随机数不随机，那么pre master secret就有可能被猜出来，那么仅适用pre master secret作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器加上pre master secret三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了




## 输入URL之后的网络操作

输入URL之后的网络操作：https://segmentfault.com/a/1190000006879700 

DNS解析过程中同时存在UDP和TCP请求：http://www.cnblogs.com/549294286/p/5172435.html

- DNS解析
  - 可扯上为什么DNS解析过程中同时存在UDP和TCP请求
  - 可扯上DNS优化，即缓存 ：浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存
  - 可扯上DNS负载均衡，即DNS重定向。CDN技术就是利用DNS重定向技术的，
- 建立TCP连接
- 发送HTTP请求
- 发送所需文件给客户端
- 释放连接
- [浏览器渲染WEB资源](https://segmentfault.com/a/1190000010298038#articleHeader5)